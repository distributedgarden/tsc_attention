{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "- Download the ECG dataset hosted on kaggle. **This step requires a Kaggle API token.**\n",
        "- Clone the project repository to access the experiment models"
      ],
      "metadata": {
        "id": "JEvTpK2o6BH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "0Vv70yggtIRc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# make sure there is a kaggle.json file\n",
        "!ls -lha kaggle.json\n",
        "\n",
        "# install the Kaggle API token\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "DwW7WKWWtJhI",
        "outputId": "5c9069b8-ef03-4fb5-9a87-210bd2b4fa04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc35c343-0e9e-40e2-906d-2e15b2e0a161\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dc35c343-0e9e-40e2-906d-2e15b2e0a161\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 67 Nov 12 01:57 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download and unzip the ECG dataset hosted on kaggle\n",
        "!kaggle datasets download -d shayanfazeli/heartbeat\n",
        "!unzip -q heartbeat.zip\n",
        "\n",
        "# clone the project github repository\n",
        "!git clone https://github.com/distributedgarden/tsc_attention.git"
      ],
      "metadata": {
        "id": "jlWuDgwUuLsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6feec373-3e08-4357-e8af-7a5cd5358183"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading heartbeat.zip to /content\n",
            " 80% 79.0M/98.8M [00:00<00:00, 95.9MB/s]\n",
            "100% 98.8M/98.8M [00:00<00:00, 126MB/s] \n",
            "Cloning into 'tsc_attention'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 74 (delta 31), reused 49 (delta 16), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (74/74), 1.67 MiB | 2.13 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Experiment\n",
        "### Description:\n",
        "Use the ECG dataset to train the basic LSTM model and evaluate its performance.\n",
        "\n",
        "\n",
        "### Steps:\n",
        "1. split the ECG data into train and test subsets\n",
        "1. preprocess the subsets (standard scaling)\n",
        "1. convert to pytorch tensors\n",
        "1. set up the LSTM model\n",
        "1. train the LSTM model\n",
        "1. evaluate performance with accuracy, F1 score, precision, recall"
      ],
      "metadata": {
        "id": "CQcNjrlX3sMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "%cd tsc_attention\n",
        "from src.models.lstm import LSTM\n",
        "from src.models.attention_lstm import AttentionLSTM\n",
        "%cd ..\n"
      ],
      "metadata": {
        "id": "M3VZ0VQNvWmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4170c860-21a8-45f0-fd4c-6dd3c5b59606"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tsc_attention\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "train_df = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
        "test_df = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
        "\n",
        "# last column is the label\n",
        "X_train = train_df.iloc[:, :-1].values\n",
        "y_train = train_df.iloc[:, -1].values\n",
        "X_test = test_df.iloc[:, :-1].values\n",
        "y_test = test_df.iloc[:, -1].values\n",
        "\n",
        "print(len(train_df))\n",
        "print(len(test_df))\n",
        "print(len(X_train[1]))\n",
        "print(X_train[1])"
      ],
      "metadata": {
        "id": "GCmYXE8euY8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3981c9-e17c-4ed1-bdf5-85e880938c45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87554\n",
            "21892\n",
            "187\n",
            "[0.96011394 0.86324787 0.46153846 0.1965812  0.0940171  0.12535612\n",
            " 0.0997151  0.08831909 0.07407407 0.08262108 0.07407407 0.06267806\n",
            " 0.06552707 0.06552707 0.06267806 0.07692308 0.07122507 0.08262108\n",
            " 0.09116809 0.09686609 0.08262108 0.08262108 0.09116809 0.10541311\n",
            " 0.12250713 0.14814815 0.18233618 0.19373219 0.21367522 0.20797721\n",
            " 0.22222222 0.25356126 0.27065527 0.28774929 0.28490028 0.29344729\n",
            " 0.25641027 0.24786325 0.18803419 0.14529915 0.10826211 0.08262108\n",
            " 0.07977208 0.07407407 0.01424501 0.01139601 0.06267806 0.05128205\n",
            " 0.05698006 0.04843305 0.02849003 0.03133903 0.07692308 0.02564103\n",
            " 0.02849003 0.03703704 0.0940171  0.08547009 0.03988604 0.05982906\n",
            " 0.07407407 0.07977208 0.09116809 0.0997151  0.10826211 0.08831909\n",
            " 0.09116809 0.06552707 0.08547009 0.08831909 0.07692308 0.08262108\n",
            " 0.09686609 0.0997151  0.13390313 0.1025641  0.03988604 0.06552707\n",
            " 0.07407407 0.08262108 0.08547009 0.05698006 0.04558405 0.1025641\n",
            " 0.03988604 0.01139601 0.01709402 0.03133903 0.00569801 0.00854701\n",
            " 0.03133903 0.05128205 0.05698006 0.08831909 0.06552707 0.01139601\n",
            " 0.05698006 0.03988604 0.03988604 0.02564103 0.002849   0.01994302\n",
            " 0.02564103 0.01139601 0.02849003 0.01994302 0.02279202 0.03418804\n",
            " 0.01424501 0.05128205 0.06837607 0.13960114 0.28774929 0.52706552\n",
            " 0.77777779 1.         0.8888889  0.49287748 0.19088319 0.08831909\n",
            " 0.06267806 0.03418804 0.         0.03418804 0.01709402 0.002849\n",
            " 0.         0.04843305 0.04843305 0.05413105 0.04273504 0.05413105\n",
            " 0.05982906 0.06267806 0.07122507 0.07692308 0.0997151  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the ECG signals\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train).float().unsqueeze(2)  # Adding channel dimension\n",
        "y_train_tensor = torch.tensor(y_train).long()\n",
        "X_test_tensor = torch.tensor(X_test).float().unsqueeze(2)  # Adding channel dimension\n",
        "y_test_tensor = torch.tensor(y_test).long()\n",
        "\n",
        "# Dataset objects\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "t_1ulp5cvQUl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd tsc_attention\n",
        "\n",
        "# set parameters\n",
        "# ECG data is univariate, so the input dimension is 1\n",
        "input_dim = 1\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = 5\n",
        "\n",
        "model = LSTM(input_dim, hidden_dim, num_layers, num_classes)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "_oNYBbJKyKyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafe72f7-2d33-49da-c681-36b7fbd239e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(1, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# evaluate\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        true_labels.append(labels)\n",
        "        predicted_labels.append(predicted)\n",
        "\n",
        "# combine lists of results\n",
        "true_labels = torch.cat(true_labels).cpu().numpy()\n",
        "predicted_labels = torch.cat(predicted_labels).cpu().numpy()\n",
        "\n",
        "# metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "lubFaOW2o0q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3097e1b9-06c8-4cd5-ca72-e324a6196c99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/2737], Loss: 0.6394\n",
            "Epoch [1/10], Step [200/2737], Loss: 0.4769\n",
            "Epoch [1/10], Step [300/2737], Loss: 0.5001\n",
            "Epoch [1/10], Step [400/2737], Loss: 0.3592\n",
            "Epoch [1/10], Step [500/2737], Loss: 0.5476\n",
            "Epoch [1/10], Step [600/2737], Loss: 0.5029\n",
            "Epoch [1/10], Step [700/2737], Loss: 0.6562\n",
            "Epoch [1/10], Step [800/2737], Loss: 0.8768\n",
            "Epoch [1/10], Step [900/2737], Loss: 0.8531\n",
            "Epoch [1/10], Step [1000/2737], Loss: 0.9320\n",
            "Epoch [1/10], Step [1100/2737], Loss: 0.6115\n",
            "Epoch [1/10], Step [1200/2737], Loss: 0.6152\n",
            "Epoch [1/10], Step [1300/2737], Loss: 0.7548\n",
            "Epoch [1/10], Step [1400/2737], Loss: 0.6999\n",
            "Epoch [1/10], Step [1500/2737], Loss: 0.9016\n",
            "Epoch [1/10], Step [1600/2737], Loss: 0.7907\n",
            "Epoch [1/10], Step [1700/2737], Loss: 0.8906\n",
            "Epoch [1/10], Step [1800/2737], Loss: 1.0107\n",
            "Epoch [1/10], Step [1900/2737], Loss: 0.5608\n",
            "Epoch [1/10], Step [2000/2737], Loss: 0.6378\n",
            "Epoch [1/10], Step [2100/2737], Loss: 0.6361\n",
            "Epoch [1/10], Step [2200/2737], Loss: 0.7101\n",
            "Epoch [1/10], Step [2300/2737], Loss: 0.7254\n",
            "Epoch [1/10], Step [2400/2737], Loss: 0.4947\n",
            "Epoch [1/10], Step [2500/2737], Loss: 0.4604\n",
            "Epoch [1/10], Step [2600/2737], Loss: 0.6333\n",
            "Epoch [1/10], Step [2700/2737], Loss: 0.3969\n",
            "Epoch [2/10], Step [100/2737], Loss: 0.5825\n",
            "Epoch [2/10], Step [200/2737], Loss: 0.4539\n",
            "Epoch [2/10], Step [300/2737], Loss: 0.3748\n",
            "Epoch [2/10], Step [400/2737], Loss: 0.5869\n",
            "Epoch [2/10], Step [500/2737], Loss: 0.4012\n",
            "Epoch [2/10], Step [600/2737], Loss: 0.6871\n",
            "Epoch [2/10], Step [700/2737], Loss: 0.6696\n",
            "Epoch [2/10], Step [800/2737], Loss: 0.4183\n",
            "Epoch [2/10], Step [900/2737], Loss: 0.3887\n",
            "Epoch [2/10], Step [1000/2737], Loss: 0.3796\n",
            "Epoch [2/10], Step [1100/2737], Loss: 0.4400\n",
            "Epoch [2/10], Step [1200/2737], Loss: 0.8675\n",
            "Epoch [2/10], Step [1300/2737], Loss: 0.3946\n",
            "Epoch [2/10], Step [1400/2737], Loss: 0.6229\n",
            "Epoch [2/10], Step [1500/2737], Loss: 0.3812\n",
            "Epoch [2/10], Step [1600/2737], Loss: 0.6741\n",
            "Epoch [2/10], Step [1700/2737], Loss: 0.4311\n",
            "Epoch [2/10], Step [1800/2737], Loss: 0.3375\n",
            "Epoch [2/10], Step [1900/2737], Loss: 0.5413\n",
            "Epoch [2/10], Step [2000/2737], Loss: 0.5334\n",
            "Epoch [2/10], Step [2100/2737], Loss: 0.3045\n",
            "Epoch [2/10], Step [2200/2737], Loss: 0.5110\n",
            "Epoch [2/10], Step [2300/2737], Loss: 0.3063\n",
            "Epoch [2/10], Step [2400/2737], Loss: 0.2053\n",
            "Epoch [2/10], Step [2500/2737], Loss: 0.1735\n",
            "Epoch [2/10], Step [2600/2737], Loss: 0.4612\n",
            "Epoch [2/10], Step [2700/2737], Loss: 0.0972\n",
            "Epoch [3/10], Step [100/2737], Loss: 0.4425\n",
            "Epoch [3/10], Step [200/2737], Loss: 0.4253\n",
            "Epoch [3/10], Step [300/2737], Loss: 0.0839\n",
            "Epoch [3/10], Step [400/2737], Loss: 0.7323\n",
            "Epoch [3/10], Step [500/2737], Loss: 0.3366\n",
            "Epoch [3/10], Step [600/2737], Loss: 0.1938\n",
            "Epoch [3/10], Step [700/2737], Loss: 0.3353\n",
            "Epoch [3/10], Step [800/2737], Loss: 0.1535\n",
            "Epoch [3/10], Step [900/2737], Loss: 0.2094\n",
            "Epoch [3/10], Step [1000/2737], Loss: 0.3234\n",
            "Epoch [3/10], Step [1100/2737], Loss: 0.2088\n",
            "Epoch [3/10], Step [1200/2737], Loss: 0.3314\n",
            "Epoch [3/10], Step [1300/2737], Loss: 0.2189\n",
            "Epoch [3/10], Step [1400/2737], Loss: 0.1775\n",
            "Epoch [3/10], Step [1500/2737], Loss: 0.2258\n",
            "Epoch [3/10], Step [1600/2737], Loss: 0.2574\n",
            "Epoch [3/10], Step [1700/2737], Loss: 0.2700\n",
            "Epoch [3/10], Step [1800/2737], Loss: 0.2889\n",
            "Epoch [3/10], Step [1900/2737], Loss: 0.3662\n",
            "Epoch [3/10], Step [2000/2737], Loss: 0.1777\n",
            "Epoch [3/10], Step [2100/2737], Loss: 0.0489\n",
            "Epoch [3/10], Step [2200/2737], Loss: 0.1813\n",
            "Epoch [3/10], Step [2300/2737], Loss: 0.1349\n",
            "Epoch [3/10], Step [2400/2737], Loss: 0.4063\n",
            "Epoch [3/10], Step [2500/2737], Loss: 0.4872\n",
            "Epoch [3/10], Step [2600/2737], Loss: 0.0662\n",
            "Epoch [3/10], Step [2700/2737], Loss: 0.1940\n",
            "Epoch [4/10], Step [100/2737], Loss: 0.4529\n",
            "Epoch [4/10], Step [200/2737], Loss: 0.2357\n",
            "Epoch [4/10], Step [300/2737], Loss: 0.1743\n",
            "Epoch [4/10], Step [400/2737], Loss: 0.5291\n",
            "Epoch [4/10], Step [500/2737], Loss: 0.4435\n",
            "Epoch [4/10], Step [600/2737], Loss: 0.2861\n",
            "Epoch [4/10], Step [700/2737], Loss: 0.4109\n",
            "Epoch [4/10], Step [800/2737], Loss: 0.3999\n",
            "Epoch [4/10], Step [900/2737], Loss: 0.1185\n",
            "Epoch [4/10], Step [1000/2737], Loss: 0.4351\n",
            "Epoch [4/10], Step [1100/2737], Loss: 0.6906\n",
            "Epoch [4/10], Step [1200/2737], Loss: 0.1759\n",
            "Epoch [4/10], Step [1300/2737], Loss: 0.4058\n",
            "Epoch [4/10], Step [1400/2737], Loss: 0.3029\n",
            "Epoch [4/10], Step [1500/2737], Loss: 0.3803\n",
            "Epoch [4/10], Step [1600/2737], Loss: 0.4064\n",
            "Epoch [4/10], Step [1700/2737], Loss: 0.2971\n",
            "Epoch [4/10], Step [1800/2737], Loss: 0.5409\n",
            "Epoch [4/10], Step [1900/2737], Loss: 0.4002\n",
            "Epoch [4/10], Step [2000/2737], Loss: 0.1573\n",
            "Epoch [4/10], Step [2100/2737], Loss: 0.1703\n",
            "Epoch [4/10], Step [2200/2737], Loss: 0.0695\n",
            "Epoch [4/10], Step [2300/2737], Loss: 0.5602\n",
            "Epoch [4/10], Step [2400/2737], Loss: 0.5851\n",
            "Epoch [4/10], Step [2500/2737], Loss: 0.2517\n",
            "Epoch [4/10], Step [2600/2737], Loss: 0.5137\n",
            "Epoch [4/10], Step [2700/2737], Loss: 0.0366\n",
            "Epoch [5/10], Step [100/2737], Loss: 0.0453\n",
            "Epoch [5/10], Step [200/2737], Loss: 0.1372\n",
            "Epoch [5/10], Step [300/2737], Loss: 0.1895\n",
            "Epoch [5/10], Step [400/2737], Loss: 0.0928\n",
            "Epoch [5/10], Step [500/2737], Loss: 0.2438\n",
            "Epoch [5/10], Step [600/2737], Loss: 0.2622\n",
            "Epoch [5/10], Step [700/2737], Loss: 0.2604\n",
            "Epoch [5/10], Step [800/2737], Loss: 0.1447\n",
            "Epoch [5/10], Step [900/2737], Loss: 0.1483\n",
            "Epoch [5/10], Step [1000/2737], Loss: 0.1106\n",
            "Epoch [5/10], Step [1100/2737], Loss: 0.0816\n",
            "Epoch [5/10], Step [1200/2737], Loss: 0.2114\n",
            "Epoch [5/10], Step [1300/2737], Loss: 0.1273\n",
            "Epoch [5/10], Step [1400/2737], Loss: 0.3383\n",
            "Epoch [5/10], Step [1500/2737], Loss: 0.1675\n",
            "Epoch [5/10], Step [1600/2737], Loss: 0.1395\n",
            "Epoch [5/10], Step [1700/2737], Loss: 0.1631\n",
            "Epoch [5/10], Step [1800/2737], Loss: 0.1109\n",
            "Epoch [5/10], Step [1900/2737], Loss: 0.2097\n",
            "Epoch [5/10], Step [2000/2737], Loss: 0.3580\n",
            "Epoch [5/10], Step [2100/2737], Loss: 0.0695\n",
            "Epoch [5/10], Step [2200/2737], Loss: 0.1821\n",
            "Epoch [5/10], Step [2300/2737], Loss: 0.2436\n",
            "Epoch [5/10], Step [2400/2737], Loss: 0.0334\n",
            "Epoch [5/10], Step [2500/2737], Loss: 0.2619\n",
            "Epoch [5/10], Step [2600/2737], Loss: 0.0571\n",
            "Epoch [5/10], Step [2700/2737], Loss: 0.4151\n",
            "Epoch [6/10], Step [100/2737], Loss: 0.2290\n",
            "Epoch [6/10], Step [200/2737], Loss: 0.0834\n",
            "Epoch [6/10], Step [300/2737], Loss: 0.0573\n",
            "Epoch [6/10], Step [400/2737], Loss: 0.2282\n",
            "Epoch [6/10], Step [500/2737], Loss: 0.2755\n",
            "Epoch [6/10], Step [600/2737], Loss: 0.1399\n",
            "Epoch [6/10], Step [700/2737], Loss: 0.2695\n",
            "Epoch [6/10], Step [800/2737], Loss: 0.0365\n",
            "Epoch [6/10], Step [900/2737], Loss: 0.2857\n",
            "Epoch [6/10], Step [1000/2737], Loss: 0.3365\n",
            "Epoch [6/10], Step [1100/2737], Loss: 0.0892\n",
            "Epoch [6/10], Step [1200/2737], Loss: 0.2078\n",
            "Epoch [6/10], Step [1300/2737], Loss: 0.0238\n",
            "Epoch [6/10], Step [1400/2737], Loss: 0.1214\n",
            "Epoch [6/10], Step [1500/2737], Loss: 0.2215\n",
            "Epoch [6/10], Step [1600/2737], Loss: 0.2910\n",
            "Epoch [6/10], Step [1700/2737], Loss: 0.2882\n",
            "Epoch [6/10], Step [1800/2737], Loss: 0.0721\n",
            "Epoch [6/10], Step [1900/2737], Loss: 0.2073\n",
            "Epoch [6/10], Step [2000/2737], Loss: 0.0217\n",
            "Epoch [6/10], Step [2100/2737], Loss: 0.2177\n",
            "Epoch [6/10], Step [2200/2737], Loss: 0.2195\n",
            "Epoch [6/10], Step [2300/2737], Loss: 0.2304\n",
            "Epoch [6/10], Step [2400/2737], Loss: 0.0673\n",
            "Epoch [6/10], Step [2500/2737], Loss: 0.4227\n",
            "Epoch [6/10], Step [2600/2737], Loss: 0.2280\n",
            "Epoch [6/10], Step [2700/2737], Loss: 0.0530\n",
            "Epoch [7/10], Step [100/2737], Loss: 0.1694\n",
            "Epoch [7/10], Step [200/2737], Loss: 0.0505\n",
            "Epoch [7/10], Step [300/2737], Loss: 0.0205\n",
            "Epoch [7/10], Step [400/2737], Loss: 0.1790\n",
            "Epoch [7/10], Step [500/2737], Loss: 0.0999\n",
            "Epoch [7/10], Step [600/2737], Loss: 0.0602\n",
            "Epoch [7/10], Step [700/2737], Loss: 0.3712\n",
            "Epoch [7/10], Step [800/2737], Loss: 0.2917\n",
            "Epoch [7/10], Step [900/2737], Loss: 0.2321\n",
            "Epoch [7/10], Step [1000/2737], Loss: 0.0205\n",
            "Epoch [7/10], Step [1100/2737], Loss: 0.0217\n",
            "Epoch [7/10], Step [1200/2737], Loss: 0.0706\n",
            "Epoch [7/10], Step [1300/2737], Loss: 0.1175\n",
            "Epoch [7/10], Step [1400/2737], Loss: 0.1431\n",
            "Epoch [7/10], Step [1500/2737], Loss: 0.0108\n",
            "Epoch [7/10], Step [1600/2737], Loss: 0.0540\n",
            "Epoch [7/10], Step [1700/2737], Loss: 0.3711\n",
            "Epoch [7/10], Step [1800/2737], Loss: 0.0630\n",
            "Epoch [7/10], Step [1900/2737], Loss: 0.2082\n",
            "Epoch [7/10], Step [2000/2737], Loss: 0.0116\n",
            "Epoch [7/10], Step [2100/2737], Loss: 0.0596\n",
            "Epoch [7/10], Step [2200/2737], Loss: 0.0113\n",
            "Epoch [7/10], Step [2300/2737], Loss: 0.1699\n",
            "Epoch [7/10], Step [2400/2737], Loss: 0.2721\n",
            "Epoch [7/10], Step [2500/2737], Loss: 0.1796\n",
            "Epoch [7/10], Step [2600/2737], Loss: 0.0452\n",
            "Epoch [7/10], Step [2700/2737], Loss: 0.0397\n",
            "Epoch [8/10], Step [100/2737], Loss: 0.0444\n",
            "Epoch [8/10], Step [200/2737], Loss: 0.0200\n",
            "Epoch [8/10], Step [300/2737], Loss: 0.1772\n",
            "Epoch [8/10], Step [400/2737], Loss: 0.0383\n",
            "Epoch [8/10], Step [500/2737], Loss: 0.1717\n",
            "Epoch [8/10], Step [600/2737], Loss: 0.0216\n",
            "Epoch [8/10], Step [700/2737], Loss: 0.0644\n",
            "Epoch [8/10], Step [800/2737], Loss: 0.0372\n",
            "Epoch [8/10], Step [900/2737], Loss: 0.0089\n",
            "Epoch [8/10], Step [1000/2737], Loss: 0.2286\n",
            "Epoch [8/10], Step [1100/2737], Loss: 0.0932\n",
            "Epoch [8/10], Step [1200/2737], Loss: 0.1013\n",
            "Epoch [8/10], Step [1300/2737], Loss: 0.0265\n",
            "Epoch [8/10], Step [1400/2737], Loss: 0.1740\n",
            "Epoch [8/10], Step [1500/2737], Loss: 0.0783\n",
            "Epoch [8/10], Step [1600/2737], Loss: 0.0671\n",
            "Epoch [8/10], Step [1700/2737], Loss: 0.0471\n",
            "Epoch [8/10], Step [1800/2737], Loss: 0.1689\n",
            "Epoch [8/10], Step [1900/2737], Loss: 0.0454\n",
            "Epoch [8/10], Step [2000/2737], Loss: 0.0415\n",
            "Epoch [8/10], Step [2100/2737], Loss: 0.0182\n",
            "Epoch [8/10], Step [2200/2737], Loss: 0.1859\n",
            "Epoch [8/10], Step [2300/2737], Loss: 0.0241\n",
            "Epoch [8/10], Step [2400/2737], Loss: 0.0751\n",
            "Epoch [8/10], Step [2500/2737], Loss: 0.0437\n",
            "Epoch [8/10], Step [2600/2737], Loss: 0.2089\n",
            "Epoch [8/10], Step [2700/2737], Loss: 0.2188\n",
            "Epoch [9/10], Step [100/2737], Loss: 0.3452\n",
            "Epoch [9/10], Step [200/2737], Loss: 0.0070\n",
            "Epoch [9/10], Step [300/2737], Loss: 0.1373\n",
            "Epoch [9/10], Step [400/2737], Loss: 0.0812\n",
            "Epoch [9/10], Step [500/2737], Loss: 0.0269\n",
            "Epoch [9/10], Step [600/2737], Loss: 0.0651\n",
            "Epoch [9/10], Step [700/2737], Loss: 0.0515\n",
            "Epoch [9/10], Step [800/2737], Loss: 0.2146\n",
            "Epoch [9/10], Step [900/2737], Loss: 0.1429\n",
            "Epoch [9/10], Step [1000/2737], Loss: 0.0580\n",
            "Epoch [9/10], Step [1100/2737], Loss: 0.0353\n",
            "Epoch [9/10], Step [1200/2737], Loss: 0.0628\n",
            "Epoch [9/10], Step [1300/2737], Loss: 0.0245\n",
            "Epoch [9/10], Step [1400/2737], Loss: 0.1539\n",
            "Epoch [9/10], Step [1500/2737], Loss: 0.0819\n",
            "Epoch [9/10], Step [1600/2737], Loss: 0.0222\n",
            "Epoch [9/10], Step [1700/2737], Loss: 0.1810\n",
            "Epoch [9/10], Step [1800/2737], Loss: 0.2335\n",
            "Epoch [9/10], Step [1900/2737], Loss: 0.1943\n",
            "Epoch [9/10], Step [2000/2737], Loss: 0.1394\n",
            "Epoch [9/10], Step [2100/2737], Loss: 0.0247\n",
            "Epoch [9/10], Step [2200/2737], Loss: 0.2622\n",
            "Epoch [9/10], Step [2300/2737], Loss: 0.0428\n",
            "Epoch [9/10], Step [2400/2737], Loss: 0.0548\n",
            "Epoch [9/10], Step [2500/2737], Loss: 0.0681\n",
            "Epoch [9/10], Step [2600/2737], Loss: 0.0946\n",
            "Epoch [9/10], Step [2700/2737], Loss: 0.0187\n",
            "Epoch [10/10], Step [100/2737], Loss: 0.0565\n",
            "Epoch [10/10], Step [200/2737], Loss: 0.2869\n",
            "Epoch [10/10], Step [300/2737], Loss: 0.0106\n",
            "Epoch [10/10], Step [400/2737], Loss: 0.0789\n",
            "Epoch [10/10], Step [500/2737], Loss: 0.0411\n",
            "Epoch [10/10], Step [600/2737], Loss: 0.0285\n",
            "Epoch [10/10], Step [700/2737], Loss: 0.4070\n",
            "Epoch [10/10], Step [800/2737], Loss: 0.0337\n",
            "Epoch [10/10], Step [900/2737], Loss: 0.0306\n",
            "Epoch [10/10], Step [1000/2737], Loss: 0.0185\n",
            "Epoch [10/10], Step [1100/2737], Loss: 0.2226\n",
            "Epoch [10/10], Step [1200/2737], Loss: 0.0597\n",
            "Epoch [10/10], Step [1300/2737], Loss: 0.0102\n",
            "Epoch [10/10], Step [1400/2737], Loss: 0.0611\n",
            "Epoch [10/10], Step [1500/2737], Loss: 0.0175\n",
            "Epoch [10/10], Step [1600/2737], Loss: 0.0148\n",
            "Epoch [10/10], Step [1700/2737], Loss: 0.0143\n",
            "Epoch [10/10], Step [1800/2737], Loss: 0.0462\n",
            "Epoch [10/10], Step [1900/2737], Loss: 0.0082\n",
            "Epoch [10/10], Step [2000/2737], Loss: 0.0740\n",
            "Epoch [10/10], Step [2100/2737], Loss: 0.0267\n",
            "Epoch [10/10], Step [2200/2737], Loss: 0.0760\n",
            "Epoch [10/10], Step [2300/2737], Loss: 0.2591\n",
            "Epoch [10/10], Step [2400/2737], Loss: 0.1074\n",
            "Epoch [10/10], Step [2500/2737], Loss: 0.0268\n",
            "Epoch [10/10], Step [2600/2737], Loss: 0.0189\n",
            "Epoch [10/10], Step [2700/2737], Loss: 0.0083\n",
            "Accuracy: 97.41%\n",
            "Precision: 0.9729\n",
            "Recall: 0.9741\n",
            "F1 Score: 0.9729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM with Attention Experiment\n",
        "### Description\n",
        "Use the ECG dataset to train the LSTM+Attention model and evaluate its performance.\n",
        "\n",
        "### Steps\n",
        "1. set up the LSTM+Attention model\n",
        "1. train the LSTM+Attention model\n",
        "1. evaluate performance with accuracy, F1 score, precision, recall"
      ],
      "metadata": {
        "id": "8H-W0Bxa4MQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd tsc_attention\n",
        "\n",
        "# set parameters\n",
        "input_dim = 1\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = 5\n",
        "\n",
        "attention_model = AttentionLSTM(input_dim, hidden_dim, num_layers, num_classes)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(attention_model.parameters(), lr=0.001)\n",
        "\n",
        "print(attention_model)"
      ],
      "metadata": {
        "id": "v7i9kqLcyKK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d697c03b-9e4d-4b95-dd6b-4d1daaa75109"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttentionLSTM(\n",
            "  (lstm): LSTM(1, 128, num_layers=2, batch_first=True)\n",
            "  (attention): SelfAttention(\n",
            "    (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    attention_model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = attention_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# evaluate\n",
        "attention_model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = attention_model(inputs)\n",
        "        probabilities = F.softmax(outputs, dim=1)  # Apply softmax to convert logits to probabilities\n",
        "        _, predicted = torch.max(probabilities, 1)\n",
        "\n",
        "        true_labels.append(labels)\n",
        "        predicted_labels.append(predicted)\n",
        "\n",
        "# combine lists of results\n",
        "true_labels = torch.cat(true_labels).cpu().numpy()\n",
        "predicted_labels = torch.cat(predicted_labels).cpu().numpy()\n",
        "\n",
        "# metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "r7Ns3KEP4ZbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056e4686-bf83-4022-e71c-9b2d4676c80b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/2737], Loss: 0.7998\n",
            "Epoch [1/10], Step [200/2737], Loss: 0.8254\n",
            "Epoch [1/10], Step [300/2737], Loss: 0.5415\n",
            "Epoch [1/10], Step [400/2737], Loss: 0.6038\n",
            "Epoch [1/10], Step [500/2737], Loss: 0.6811\n",
            "Epoch [1/10], Step [600/2737], Loss: 0.6980\n",
            "Epoch [1/10], Step [700/2737], Loss: 0.4894\n",
            "Epoch [1/10], Step [800/2737], Loss: 0.4802\n",
            "Epoch [1/10], Step [900/2737], Loss: 0.9260\n",
            "Epoch [1/10], Step [1000/2737], Loss: 0.7221\n",
            "Epoch [1/10], Step [1100/2737], Loss: 0.8674\n",
            "Epoch [1/10], Step [1200/2737], Loss: 0.3706\n",
            "Epoch [1/10], Step [1300/2737], Loss: 0.7267\n",
            "Epoch [1/10], Step [1400/2737], Loss: 0.5524\n",
            "Epoch [1/10], Step [1500/2737], Loss: 0.3996\n",
            "Epoch [1/10], Step [1600/2737], Loss: 0.8409\n",
            "Epoch [1/10], Step [1700/2737], Loss: 0.6384\n",
            "Epoch [1/10], Step [1800/2737], Loss: 0.7545\n",
            "Epoch [1/10], Step [1900/2737], Loss: 0.5801\n",
            "Epoch [1/10], Step [2000/2737], Loss: 0.8985\n",
            "Epoch [1/10], Step [2100/2737], Loss: 0.4103\n",
            "Epoch [1/10], Step [2200/2737], Loss: 1.0336\n",
            "Epoch [1/10], Step [2300/2737], Loss: 0.3897\n",
            "Epoch [1/10], Step [2400/2737], Loss: 0.4847\n",
            "Epoch [1/10], Step [2500/2737], Loss: 0.4367\n",
            "Epoch [1/10], Step [2600/2737], Loss: 0.9316\n",
            "Epoch [1/10], Step [2700/2737], Loss: 0.2858\n",
            "Epoch [2/10], Step [100/2737], Loss: 0.2942\n",
            "Epoch [2/10], Step [200/2737], Loss: 0.2546\n",
            "Epoch [2/10], Step [300/2737], Loss: 0.2966\n",
            "Epoch [2/10], Step [400/2737], Loss: 0.1940\n",
            "Epoch [2/10], Step [500/2737], Loss: 0.2503\n",
            "Epoch [2/10], Step [600/2737], Loss: 0.1224\n",
            "Epoch [2/10], Step [700/2737], Loss: 0.2002\n",
            "Epoch [2/10], Step [800/2737], Loss: 0.3515\n",
            "Epoch [2/10], Step [900/2737], Loss: 0.4596\n",
            "Epoch [2/10], Step [1000/2737], Loss: 0.4876\n",
            "Epoch [2/10], Step [1100/2737], Loss: 0.0627\n",
            "Epoch [2/10], Step [1200/2737], Loss: 0.4618\n",
            "Epoch [2/10], Step [1300/2737], Loss: 0.2055\n",
            "Epoch [2/10], Step [1400/2737], Loss: 0.4700\n",
            "Epoch [2/10], Step [1500/2737], Loss: 0.5954\n",
            "Epoch [2/10], Step [1600/2737], Loss: 0.7318\n",
            "Epoch [2/10], Step [1700/2737], Loss: 0.4305\n",
            "Epoch [2/10], Step [1800/2737], Loss: 0.2788\n",
            "Epoch [2/10], Step [1900/2737], Loss: 0.1772\n",
            "Epoch [2/10], Step [2000/2737], Loss: 0.1689\n",
            "Epoch [2/10], Step [2100/2737], Loss: 0.0640\n",
            "Epoch [2/10], Step [2200/2737], Loss: 0.3825\n",
            "Epoch [2/10], Step [2300/2737], Loss: 0.2597\n",
            "Epoch [2/10], Step [2400/2737], Loss: 0.1564\n",
            "Epoch [2/10], Step [2500/2737], Loss: 0.2169\n",
            "Epoch [2/10], Step [2600/2737], Loss: 0.5840\n",
            "Epoch [2/10], Step [2700/2737], Loss: 0.3775\n",
            "Epoch [3/10], Step [100/2737], Loss: 0.4490\n",
            "Epoch [3/10], Step [200/2737], Loss: 0.0739\n",
            "Epoch [3/10], Step [300/2737], Loss: 0.2378\n",
            "Epoch [3/10], Step [400/2737], Loss: 0.1169\n",
            "Epoch [3/10], Step [500/2737], Loss: 0.3531\n",
            "Epoch [3/10], Step [600/2737], Loss: 0.3923\n",
            "Epoch [3/10], Step [700/2737], Loss: 0.2010\n",
            "Epoch [3/10], Step [800/2737], Loss: 0.4275\n",
            "Epoch [3/10], Step [900/2737], Loss: 0.3764\n",
            "Epoch [3/10], Step [1000/2737], Loss: 0.3568\n",
            "Epoch [3/10], Step [1100/2737], Loss: 0.2359\n",
            "Epoch [3/10], Step [1200/2737], Loss: 0.0813\n",
            "Epoch [3/10], Step [1300/2737], Loss: 0.2865\n",
            "Epoch [3/10], Step [1400/2737], Loss: 0.2475\n",
            "Epoch [3/10], Step [1500/2737], Loss: 0.3722\n",
            "Epoch [3/10], Step [1600/2737], Loss: 0.1605\n",
            "Epoch [3/10], Step [1700/2737], Loss: 0.0743\n",
            "Epoch [3/10], Step [1800/2737], Loss: 0.2261\n",
            "Epoch [3/10], Step [1900/2737], Loss: 0.3489\n",
            "Epoch [3/10], Step [2000/2737], Loss: 0.3585\n",
            "Epoch [3/10], Step [2100/2737], Loss: 0.3813\n",
            "Epoch [3/10], Step [2200/2737], Loss: 0.3107\n",
            "Epoch [3/10], Step [2300/2737], Loss: 0.2603\n",
            "Epoch [3/10], Step [2400/2737], Loss: 0.3832\n",
            "Epoch [3/10], Step [2500/2737], Loss: 0.4480\n",
            "Epoch [3/10], Step [2600/2737], Loss: 0.3829\n",
            "Epoch [3/10], Step [2700/2737], Loss: 0.2467\n",
            "Epoch [4/10], Step [100/2737], Loss: 0.3009\n",
            "Epoch [4/10], Step [200/2737], Loss: 0.3466\n",
            "Epoch [4/10], Step [300/2737], Loss: 0.0371\n",
            "Epoch [4/10], Step [400/2737], Loss: 0.3695\n",
            "Epoch [4/10], Step [500/2737], Loss: 0.1360\n",
            "Epoch [4/10], Step [600/2737], Loss: 0.2505\n",
            "Epoch [4/10], Step [700/2737], Loss: 0.4696\n",
            "Epoch [4/10], Step [800/2737], Loss: 0.1700\n",
            "Epoch [4/10], Step [900/2737], Loss: 0.2937\n",
            "Epoch [4/10], Step [1000/2737], Loss: 0.0946\n",
            "Epoch [4/10], Step [1100/2737], Loss: 0.2279\n",
            "Epoch [4/10], Step [1200/2737], Loss: 0.2496\n",
            "Epoch [4/10], Step [1300/2737], Loss: 0.0967\n",
            "Epoch [4/10], Step [1400/2737], Loss: 0.2339\n",
            "Epoch [4/10], Step [1500/2737], Loss: 0.2630\n",
            "Epoch [4/10], Step [1600/2737], Loss: 0.1937\n",
            "Epoch [4/10], Step [1700/2737], Loss: 0.1651\n",
            "Epoch [4/10], Step [1800/2737], Loss: 0.2947\n",
            "Epoch [4/10], Step [1900/2737], Loss: 0.6514\n",
            "Epoch [4/10], Step [2000/2737], Loss: 0.0844\n",
            "Epoch [4/10], Step [2100/2737], Loss: 0.2915\n",
            "Epoch [4/10], Step [2200/2737], Loss: 0.2801\n",
            "Epoch [4/10], Step [2300/2737], Loss: 0.1539\n",
            "Epoch [4/10], Step [2400/2737], Loss: 0.3540\n",
            "Epoch [4/10], Step [2500/2737], Loss: 0.2216\n",
            "Epoch [4/10], Step [2600/2737], Loss: 0.3239\n",
            "Epoch [4/10], Step [2700/2737], Loss: 0.1651\n",
            "Epoch [5/10], Step [100/2737], Loss: 0.2528\n",
            "Epoch [5/10], Step [200/2737], Loss: 0.2262\n",
            "Epoch [5/10], Step [300/2737], Loss: 0.3145\n",
            "Epoch [5/10], Step [400/2737], Loss: 0.2690\n",
            "Epoch [5/10], Step [500/2737], Loss: 0.2178\n",
            "Epoch [5/10], Step [600/2737], Loss: 0.1521\n",
            "Epoch [5/10], Step [700/2737], Loss: 0.1703\n",
            "Epoch [5/10], Step [800/2737], Loss: 0.2576\n",
            "Epoch [5/10], Step [900/2737], Loss: 0.1548\n",
            "Epoch [5/10], Step [1000/2737], Loss: 0.0396\n",
            "Epoch [5/10], Step [1100/2737], Loss: 0.1651\n",
            "Epoch [5/10], Step [1200/2737], Loss: 0.1911\n",
            "Epoch [5/10], Step [1300/2737], Loss: 0.2397\n",
            "Epoch [5/10], Step [1400/2737], Loss: 0.3988\n",
            "Epoch [5/10], Step [1500/2737], Loss: 0.1742\n",
            "Epoch [5/10], Step [1600/2737], Loss: 0.4419\n",
            "Epoch [5/10], Step [1700/2737], Loss: 0.2214\n",
            "Epoch [5/10], Step [1800/2737], Loss: 0.2068\n",
            "Epoch [5/10], Step [1900/2737], Loss: 0.2772\n",
            "Epoch [5/10], Step [2000/2737], Loss: 0.4484\n",
            "Epoch [5/10], Step [2100/2737], Loss: 0.0443\n",
            "Epoch [5/10], Step [2200/2737], Loss: 0.2224\n",
            "Epoch [5/10], Step [2300/2737], Loss: 0.1264\n",
            "Epoch [5/10], Step [2400/2737], Loss: 0.1426\n",
            "Epoch [5/10], Step [2500/2737], Loss: 0.1727\n",
            "Epoch [5/10], Step [2600/2737], Loss: 0.1593\n",
            "Epoch [5/10], Step [2700/2737], Loss: 0.2095\n",
            "Epoch [6/10], Step [100/2737], Loss: 0.0321\n",
            "Epoch [6/10], Step [200/2737], Loss: 0.1394\n",
            "Epoch [6/10], Step [300/2737], Loss: 0.1685\n",
            "Epoch [6/10], Step [400/2737], Loss: 0.1323\n",
            "Epoch [6/10], Step [500/2737], Loss: 0.3188\n",
            "Epoch [6/10], Step [600/2737], Loss: 0.2637\n",
            "Epoch [6/10], Step [700/2737], Loss: 0.3183\n",
            "Epoch [6/10], Step [800/2737], Loss: 0.1403\n",
            "Epoch [6/10], Step [900/2737], Loss: 0.1727\n",
            "Epoch [6/10], Step [1000/2737], Loss: 0.1679\n",
            "Epoch [6/10], Step [1100/2737], Loss: 0.4855\n",
            "Epoch [6/10], Step [1200/2737], Loss: 0.1333\n",
            "Epoch [6/10], Step [1300/2737], Loss: 0.1943\n",
            "Epoch [6/10], Step [1400/2737], Loss: 0.2432\n",
            "Epoch [6/10], Step [1500/2737], Loss: 0.1894\n",
            "Epoch [6/10], Step [1600/2737], Loss: 0.1579\n",
            "Epoch [6/10], Step [1700/2737], Loss: 0.0466\n",
            "Epoch [6/10], Step [1800/2737], Loss: 0.1995\n",
            "Epoch [6/10], Step [1900/2737], Loss: 0.0456\n",
            "Epoch [6/10], Step [2000/2737], Loss: 0.4061\n",
            "Epoch [6/10], Step [2100/2737], Loss: 0.3411\n",
            "Epoch [6/10], Step [2200/2737], Loss: 0.1295\n",
            "Epoch [6/10], Step [2300/2737], Loss: 0.2826\n",
            "Epoch [6/10], Step [2400/2737], Loss: 0.3178\n",
            "Epoch [6/10], Step [2500/2737], Loss: 0.2308\n",
            "Epoch [6/10], Step [2600/2737], Loss: 0.0907\n",
            "Epoch [6/10], Step [2700/2737], Loss: 0.2018\n",
            "Epoch [7/10], Step [100/2737], Loss: 0.3299\n",
            "Epoch [7/10], Step [200/2737], Loss: 0.1860\n",
            "Epoch [7/10], Step [300/2737], Loss: 0.1597\n",
            "Epoch [7/10], Step [400/2737], Loss: 0.3693\n",
            "Epoch [7/10], Step [500/2737], Loss: 0.0334\n",
            "Epoch [7/10], Step [600/2737], Loss: 0.2711\n",
            "Epoch [7/10], Step [700/2737], Loss: 0.2521\n",
            "Epoch [7/10], Step [800/2737], Loss: 0.1557\n",
            "Epoch [7/10], Step [900/2737], Loss: 0.3291\n",
            "Epoch [7/10], Step [1000/2737], Loss: 0.2187\n",
            "Epoch [7/10], Step [1100/2737], Loss: 0.0398\n",
            "Epoch [7/10], Step [1200/2737], Loss: 0.2622\n",
            "Epoch [7/10], Step [1300/2737], Loss: 0.2036\n",
            "Epoch [7/10], Step [1400/2737], Loss: 0.1464\n",
            "Epoch [7/10], Step [1500/2737], Loss: 0.0817\n",
            "Epoch [7/10], Step [1600/2737], Loss: 0.4825\n",
            "Epoch [7/10], Step [1700/2737], Loss: 0.0426\n",
            "Epoch [7/10], Step [1800/2737], Loss: 0.3999\n",
            "Epoch [7/10], Step [1900/2737], Loss: 0.1754\n",
            "Epoch [7/10], Step [2000/2737], Loss: 0.4290\n",
            "Epoch [7/10], Step [2100/2737], Loss: 0.2486\n",
            "Epoch [7/10], Step [2200/2737], Loss: 0.0515\n",
            "Epoch [7/10], Step [2300/2737], Loss: 0.3142\n",
            "Epoch [7/10], Step [2400/2737], Loss: 0.0731\n",
            "Epoch [7/10], Step [2500/2737], Loss: 0.1453\n",
            "Epoch [7/10], Step [2600/2737], Loss: 0.0311\n",
            "Epoch [7/10], Step [2700/2737], Loss: 0.0519\n",
            "Epoch [8/10], Step [100/2737], Loss: 0.2653\n",
            "Epoch [8/10], Step [200/2737], Loss: 0.4755\n",
            "Epoch [8/10], Step [300/2737], Loss: 0.1921\n",
            "Epoch [8/10], Step [400/2737], Loss: 0.0493\n",
            "Epoch [8/10], Step [500/2737], Loss: 0.1863\n",
            "Epoch [8/10], Step [600/2737], Loss: 0.2593\n",
            "Epoch [8/10], Step [700/2737], Loss: 0.0635\n",
            "Epoch [8/10], Step [800/2737], Loss: 0.2646\n",
            "Epoch [8/10], Step [900/2737], Loss: 0.1842\n",
            "Epoch [8/10], Step [1000/2737], Loss: 0.0876\n",
            "Epoch [8/10], Step [1100/2737], Loss: 0.3706\n",
            "Epoch [8/10], Step [1200/2737], Loss: 0.2443\n",
            "Epoch [8/10], Step [1300/2737], Loss: 0.1657\n",
            "Epoch [8/10], Step [1400/2737], Loss: 0.1976\n",
            "Epoch [8/10], Step [1500/2737], Loss: 0.0911\n",
            "Epoch [8/10], Step [1600/2737], Loss: 0.0848\n",
            "Epoch [8/10], Step [1700/2737], Loss: 0.1868\n",
            "Epoch [8/10], Step [1800/2737], Loss: 0.0489\n",
            "Epoch [8/10], Step [1900/2737], Loss: 0.2216\n",
            "Epoch [8/10], Step [2000/2737], Loss: 0.0454\n",
            "Epoch [8/10], Step [2100/2737], Loss: 0.1224\n",
            "Epoch [8/10], Step [2200/2737], Loss: 0.1477\n",
            "Epoch [8/10], Step [2300/2737], Loss: 0.2071\n",
            "Epoch [8/10], Step [2400/2737], Loss: 0.0820\n",
            "Epoch [8/10], Step [2500/2737], Loss: 0.3008\n",
            "Epoch [8/10], Step [2600/2737], Loss: 0.1385\n",
            "Epoch [8/10], Step [2700/2737], Loss: 0.4661\n",
            "Epoch [9/10], Step [100/2737], Loss: 0.1255\n",
            "Epoch [9/10], Step [200/2737], Loss: 0.3229\n",
            "Epoch [9/10], Step [300/2737], Loss: 0.0657\n",
            "Epoch [9/10], Step [400/2737], Loss: 0.2466\n",
            "Epoch [9/10], Step [500/2737], Loss: 0.1170\n",
            "Epoch [9/10], Step [600/2737], Loss: 0.2810\n",
            "Epoch [9/10], Step [700/2737], Loss: 0.1324\n",
            "Epoch [9/10], Step [800/2737], Loss: 0.1010\n",
            "Epoch [9/10], Step [900/2737], Loss: 0.2551\n",
            "Epoch [9/10], Step [1000/2737], Loss: 0.2697\n",
            "Epoch [9/10], Step [1100/2737], Loss: 0.3538\n",
            "Epoch [9/10], Step [1200/2737], Loss: 0.0417\n",
            "Epoch [9/10], Step [1300/2737], Loss: 0.1154\n",
            "Epoch [9/10], Step [1400/2737], Loss: 0.1347\n",
            "Epoch [9/10], Step [1500/2737], Loss: 0.1394\n",
            "Epoch [9/10], Step [1600/2737], Loss: 0.0444\n",
            "Epoch [9/10], Step [1700/2737], Loss: 0.0612\n",
            "Epoch [9/10], Step [1800/2737], Loss: 0.1262\n",
            "Epoch [9/10], Step [1900/2737], Loss: 0.3167\n",
            "Epoch [9/10], Step [2000/2737], Loss: 0.1739\n",
            "Epoch [9/10], Step [2100/2737], Loss: 0.0606\n",
            "Epoch [9/10], Step [2200/2737], Loss: 0.2334\n",
            "Epoch [9/10], Step [2300/2737], Loss: 0.0758\n",
            "Epoch [9/10], Step [2400/2737], Loss: 0.0182\n",
            "Epoch [9/10], Step [2500/2737], Loss: 0.1971\n",
            "Epoch [9/10], Step [2600/2737], Loss: 0.2322\n",
            "Epoch [9/10], Step [2700/2737], Loss: 0.1950\n",
            "Epoch [10/10], Step [100/2737], Loss: 0.0626\n",
            "Epoch [10/10], Step [200/2737], Loss: 0.0480\n",
            "Epoch [10/10], Step [300/2737], Loss: 0.0522\n",
            "Epoch [10/10], Step [400/2737], Loss: 0.1065\n",
            "Epoch [10/10], Step [500/2737], Loss: 0.0887\n",
            "Epoch [10/10], Step [600/2737], Loss: 0.2791\n",
            "Epoch [10/10], Step [700/2737], Loss: 0.2368\n",
            "Epoch [10/10], Step [800/2737], Loss: 0.0083\n",
            "Epoch [10/10], Step [900/2737], Loss: 0.0817\n",
            "Epoch [10/10], Step [1000/2737], Loss: 0.0305\n",
            "Epoch [10/10], Step [1100/2737], Loss: 0.3740\n",
            "Epoch [10/10], Step [1200/2737], Loss: 0.0266\n",
            "Epoch [10/10], Step [1300/2737], Loss: 0.3313\n",
            "Epoch [10/10], Step [1400/2737], Loss: 0.0869\n",
            "Epoch [10/10], Step [1500/2737], Loss: 0.0199\n",
            "Epoch [10/10], Step [1600/2737], Loss: 0.0525\n",
            "Epoch [10/10], Step [1700/2737], Loss: 0.0805\n",
            "Epoch [10/10], Step [1800/2737], Loss: 0.1132\n",
            "Epoch [10/10], Step [1900/2737], Loss: 0.4192\n",
            "Epoch [10/10], Step [2000/2737], Loss: 0.2449\n",
            "Epoch [10/10], Step [2100/2737], Loss: 0.2876\n",
            "Epoch [10/10], Step [2200/2737], Loss: 0.1973\n",
            "Epoch [10/10], Step [2300/2737], Loss: 0.0177\n",
            "Epoch [10/10], Step [2400/2737], Loss: 0.0663\n",
            "Epoch [10/10], Step [2500/2737], Loss: 0.0094\n",
            "Epoch [10/10], Step [2600/2737], Loss: 0.3859\n",
            "Epoch [10/10], Step [2700/2737], Loss: 0.2384\n",
            "Accuracy: 96.25%\n",
            "Precision: 0.9606\n",
            "Recall: 0.9625\n",
            "F1 Score: 0.9607\n"
          ]
        }
      ]
    }
  ]
}
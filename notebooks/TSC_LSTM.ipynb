{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "- Download the ECG dataset hosted on kaggle. **This step requires a Kaggle API token.**\n",
        "- Clone the project repository to access the experiment models"
      ],
      "metadata": {
        "id": "JEvTpK2o6BH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "0Vv70yggtIRc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# make sure there is a kaggle.json file\n",
        "!ls -lha kaggle.json\n",
        "\n",
        "# install the Kaggle API token\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "DwW7WKWWtJhI",
        "outputId": "528eb0b1-ebc1-4ad5-e69c-db05c9a88a99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b930d6d7-c17d-4872-98fa-9d29bfb5bb68\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b930d6d7-c17d-4872-98fa-9d29bfb5bb68\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (4).json\n",
            "-rw-r--r-- 1 root root 67 Nov 11 00:26 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download and unzip the ECG dataset hosted on kaggle\n",
        "!kaggle datasets download -d shayanfazeli/heartbeat\n",
        "!unzip -q heartbeat.zip\n",
        "\n",
        "# clone the project github repository\n",
        "!git clone https://github.com/distributedgarden/tsc_attention.git"
      ],
      "metadata": {
        "id": "jlWuDgwUuLsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1bb5f9a-60a2-4f2f-f855-dfe86d1d9fcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heartbeat.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace mitbih_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace mitbih_train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ptbdb_abnormal.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace ptbdb_normal.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "fatal: destination path 'tsc_attention' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Experiment\n",
        "### Description:\n",
        "Use the ECG dataset to train the basic LSTM model and evaluate its performance.\n",
        "\n",
        "\n",
        "### Steps:\n",
        "1. split the ECG data into train and test subsets\n",
        "1. preprocess the subsets (standard scaling)\n",
        "1. convert to pytorch tensors\n",
        "1. set up the LSTM model\n",
        "1. train the LSTM model\n",
        "1. evaluate performance with accuracy, F1 score, precision, recall"
      ],
      "metadata": {
        "id": "CQcNjrlX3sMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "%cd tsc_attention\n",
        "from src.models.lstm import LSTM\n",
        "from src.models.attention_lstm import AttentionLSTM\n",
        "%cd ..\n"
      ],
      "metadata": {
        "id": "M3VZ0VQNvWmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e085c99-fed2-441f-9785-59bfd89abe9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tsc_attention\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "train_df = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
        "test_df = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
        "\n",
        "# last column is the label\n",
        "X_train = train_df.iloc[:, :-1].values\n",
        "y_train = train_df.iloc[:, -1].values\n",
        "X_test = test_df.iloc[:, :-1].values\n",
        "y_test = test_df.iloc[:, -1].values\n",
        "\n",
        "print(len(train_df))\n",
        "print(len(test_df))\n",
        "print(len(X_train[1]))\n",
        "print(X_train[1])"
      ],
      "metadata": {
        "id": "GCmYXE8euY8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82137558-8540-4ead-e6f5-2615a72bef91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87554\n",
            "21892\n",
            "187\n",
            "[0.96011394 0.86324787 0.46153846 0.1965812  0.0940171  0.12535612\n",
            " 0.0997151  0.08831909 0.07407407 0.08262108 0.07407407 0.06267806\n",
            " 0.06552707 0.06552707 0.06267806 0.07692308 0.07122507 0.08262108\n",
            " 0.09116809 0.09686609 0.08262108 0.08262108 0.09116809 0.10541311\n",
            " 0.12250713 0.14814815 0.18233618 0.19373219 0.21367522 0.20797721\n",
            " 0.22222222 0.25356126 0.27065527 0.28774929 0.28490028 0.29344729\n",
            " 0.25641027 0.24786325 0.18803419 0.14529915 0.10826211 0.08262108\n",
            " 0.07977208 0.07407407 0.01424501 0.01139601 0.06267806 0.05128205\n",
            " 0.05698006 0.04843305 0.02849003 0.03133903 0.07692308 0.02564103\n",
            " 0.02849003 0.03703704 0.0940171  0.08547009 0.03988604 0.05982906\n",
            " 0.07407407 0.07977208 0.09116809 0.0997151  0.10826211 0.08831909\n",
            " 0.09116809 0.06552707 0.08547009 0.08831909 0.07692308 0.08262108\n",
            " 0.09686609 0.0997151  0.13390313 0.1025641  0.03988604 0.06552707\n",
            " 0.07407407 0.08262108 0.08547009 0.05698006 0.04558405 0.1025641\n",
            " 0.03988604 0.01139601 0.01709402 0.03133903 0.00569801 0.00854701\n",
            " 0.03133903 0.05128205 0.05698006 0.08831909 0.06552707 0.01139601\n",
            " 0.05698006 0.03988604 0.03988604 0.02564103 0.002849   0.01994302\n",
            " 0.02564103 0.01139601 0.02849003 0.01994302 0.02279202 0.03418804\n",
            " 0.01424501 0.05128205 0.06837607 0.13960114 0.28774929 0.52706552\n",
            " 0.77777779 1.         0.8888889  0.49287748 0.19088319 0.08831909\n",
            " 0.06267806 0.03418804 0.         0.03418804 0.01709402 0.002849\n",
            " 0.         0.04843305 0.04843305 0.05413105 0.04273504 0.05413105\n",
            " 0.05982906 0.06267806 0.07122507 0.07692308 0.0997151  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the ECG signals\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train).float().unsqueeze(2)  # Adding channel dimension\n",
        "y_train_tensor = torch.tensor(y_train).long()\n",
        "X_test_tensor = torch.tensor(X_test).float().unsqueeze(2)  # Adding channel dimension\n",
        "y_test_tensor = torch.tensor(y_test).long()\n",
        "\n",
        "# Dataset objects\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "t_1ulp5cvQUl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd tsc_attention\n",
        "\n",
        "# set parameters\n",
        "# ECG data is univariate, so the input dimension is 1\n",
        "input_dim = 1\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = 5\n",
        "\n",
        "model = LSTM(input_dim, hidden_dim, num_layers, num_classes)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "_oNYBbJKyKyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b3a5e45-4385-4d5d-90ff-4a17e13f266b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(1, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# evaluate\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        true_labels.append(labels)\n",
        "        predicted_labels.append(predicted)\n",
        "\n",
        "# combine lists of results\n",
        "true_labels = torch.cat(true_labels).cpu().numpy()\n",
        "predicted_labels = torch.cat(predicted_labels).cpu().numpy()\n",
        "\n",
        "# metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "lubFaOW2o0q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e602127a-fcb0-41f9-9bd9-d36c38a8a654"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/2737], Loss: 0.9022\n",
            "Epoch [1/10], Step [200/2737], Loss: 0.9652\n",
            "Epoch [1/10], Step [300/2737], Loss: 0.3407\n",
            "Epoch [1/10], Step [400/2737], Loss: 0.8190\n",
            "Epoch [1/10], Step [500/2737], Loss: 0.6633\n",
            "Epoch [1/10], Step [600/2737], Loss: 0.6305\n",
            "Epoch [1/10], Step [700/2737], Loss: 0.7176\n",
            "Epoch [1/10], Step [800/2737], Loss: 0.4364\n",
            "Epoch [1/10], Step [900/2737], Loss: 0.1948\n",
            "Epoch [1/10], Step [1000/2737], Loss: 0.5222\n",
            "Epoch [1/10], Step [1100/2737], Loss: 0.7874\n",
            "Epoch [1/10], Step [1200/2737], Loss: 0.3346\n",
            "Epoch [1/10], Step [1300/2737], Loss: 0.6266\n",
            "Epoch [1/10], Step [1400/2737], Loss: 0.4441\n",
            "Epoch [1/10], Step [1500/2737], Loss: 0.2740\n",
            "Epoch [1/10], Step [1600/2737], Loss: 0.6414\n",
            "Epoch [1/10], Step [1700/2737], Loss: 0.3938\n",
            "Epoch [1/10], Step [1800/2737], Loss: 0.7288\n",
            "Epoch [1/10], Step [1900/2737], Loss: 0.6784\n",
            "Epoch [1/10], Step [2000/2737], Loss: 0.4890\n",
            "Epoch [1/10], Step [2100/2737], Loss: 0.7913\n",
            "Epoch [1/10], Step [2200/2737], Loss: 0.4953\n",
            "Epoch [1/10], Step [2300/2737], Loss: 0.3247\n",
            "Epoch [1/10], Step [2400/2737], Loss: 0.3833\n",
            "Epoch [1/10], Step [2500/2737], Loss: 0.4701\n",
            "Epoch [1/10], Step [2600/2737], Loss: 0.1810\n",
            "Epoch [1/10], Step [2700/2737], Loss: 0.5317\n",
            "Epoch [2/10], Step [100/2737], Loss: 0.2006\n",
            "Epoch [2/10], Step [200/2737], Loss: 0.4126\n",
            "Epoch [2/10], Step [300/2737], Loss: 0.3209\n",
            "Epoch [2/10], Step [400/2737], Loss: 0.3540\n",
            "Epoch [2/10], Step [500/2737], Loss: 0.2008\n",
            "Epoch [2/10], Step [600/2737], Loss: 0.2709\n",
            "Epoch [2/10], Step [700/2737], Loss: 0.3963\n",
            "Epoch [2/10], Step [800/2737], Loss: 0.4750\n",
            "Epoch [2/10], Step [900/2737], Loss: 0.1807\n",
            "Epoch [2/10], Step [1000/2737], Loss: 0.5836\n",
            "Epoch [2/10], Step [1100/2737], Loss: 0.1736\n",
            "Epoch [2/10], Step [1200/2737], Loss: 0.8628\n",
            "Epoch [2/10], Step [1300/2737], Loss: 0.4880\n",
            "Epoch [2/10], Step [1400/2737], Loss: 0.3409\n",
            "Epoch [2/10], Step [1500/2737], Loss: 0.1330\n",
            "Epoch [2/10], Step [1600/2737], Loss: 0.5373\n",
            "Epoch [2/10], Step [1700/2737], Loss: 0.3854\n",
            "Epoch [2/10], Step [1800/2737], Loss: 0.3361\n",
            "Epoch [2/10], Step [1900/2737], Loss: 0.2765\n",
            "Epoch [2/10], Step [2000/2737], Loss: 0.2448\n",
            "Epoch [2/10], Step [2100/2737], Loss: 0.1773\n",
            "Epoch [2/10], Step [2200/2737], Loss: 0.0983\n",
            "Epoch [2/10], Step [2300/2737], Loss: 0.3421\n",
            "Epoch [2/10], Step [2400/2737], Loss: 0.6949\n",
            "Epoch [2/10], Step [2500/2737], Loss: 0.1721\n",
            "Epoch [2/10], Step [2600/2737], Loss: 0.2502\n",
            "Epoch [2/10], Step [2700/2737], Loss: 0.3113\n",
            "Epoch [3/10], Step [100/2737], Loss: 0.4859\n",
            "Epoch [3/10], Step [200/2737], Loss: 0.3523\n",
            "Epoch [3/10], Step [300/2737], Loss: 0.5059\n",
            "Epoch [3/10], Step [400/2737], Loss: 0.4604\n",
            "Epoch [3/10], Step [500/2737], Loss: 0.2966\n",
            "Epoch [3/10], Step [600/2737], Loss: 0.6772\n",
            "Epoch [3/10], Step [700/2737], Loss: 0.4208\n",
            "Epoch [3/10], Step [800/2737], Loss: 0.3657\n",
            "Epoch [3/10], Step [900/2737], Loss: 0.1652\n",
            "Epoch [3/10], Step [1000/2737], Loss: 0.1937\n",
            "Epoch [3/10], Step [1100/2737], Loss: 0.4318\n",
            "Epoch [3/10], Step [1200/2737], Loss: 0.4213\n",
            "Epoch [3/10], Step [1300/2737], Loss: 0.3765\n",
            "Epoch [3/10], Step [1400/2737], Loss: 0.3148\n",
            "Epoch [3/10], Step [1500/2737], Loss: 0.5775\n",
            "Epoch [3/10], Step [1600/2737], Loss: 0.3464\n",
            "Epoch [3/10], Step [1700/2737], Loss: 0.1848\n",
            "Epoch [3/10], Step [1800/2737], Loss: 0.1879\n",
            "Epoch [3/10], Step [1900/2737], Loss: 0.1763\n",
            "Epoch [3/10], Step [2000/2737], Loss: 0.1496\n",
            "Epoch [3/10], Step [2100/2737], Loss: 0.5291\n",
            "Epoch [3/10], Step [2200/2737], Loss: 0.1298\n",
            "Epoch [3/10], Step [2300/2737], Loss: 0.2539\n",
            "Epoch [3/10], Step [2400/2737], Loss: 0.1062\n",
            "Epoch [3/10], Step [2500/2737], Loss: 0.3491\n",
            "Epoch [3/10], Step [2600/2737], Loss: 0.3361\n",
            "Epoch [3/10], Step [2700/2737], Loss: 0.3918\n",
            "Epoch [4/10], Step [100/2737], Loss: 0.2847\n",
            "Epoch [4/10], Step [200/2737], Loss: 0.0872\n",
            "Epoch [4/10], Step [300/2737], Loss: 0.2611\n",
            "Epoch [4/10], Step [400/2737], Loss: 0.3266\n",
            "Epoch [4/10], Step [500/2737], Loss: 0.1114\n",
            "Epoch [4/10], Step [600/2737], Loss: 0.1137\n",
            "Epoch [4/10], Step [700/2737], Loss: 0.3887\n",
            "Epoch [4/10], Step [800/2737], Loss: 0.5556\n",
            "Epoch [4/10], Step [900/2737], Loss: 0.2009\n",
            "Epoch [4/10], Step [1000/2737], Loss: 0.1756\n",
            "Epoch [4/10], Step [1100/2737], Loss: 0.0715\n",
            "Epoch [4/10], Step [1200/2737], Loss: 0.2242\n",
            "Epoch [4/10], Step [1300/2737], Loss: 0.1498\n",
            "Epoch [4/10], Step [1400/2737], Loss: 0.4069\n",
            "Epoch [4/10], Step [1500/2737], Loss: 0.2208\n",
            "Epoch [4/10], Step [1600/2737], Loss: 0.1877\n",
            "Epoch [4/10], Step [1700/2737], Loss: 0.4449\n",
            "Epoch [4/10], Step [1800/2737], Loss: 0.3662\n",
            "Epoch [4/10], Step [1900/2737], Loss: 0.0615\n",
            "Epoch [4/10], Step [2000/2737], Loss: 0.2491\n",
            "Epoch [4/10], Step [2100/2737], Loss: 0.3308\n",
            "Epoch [4/10], Step [2200/2737], Loss: 0.4668\n",
            "Epoch [4/10], Step [2300/2737], Loss: 0.2423\n",
            "Epoch [4/10], Step [2400/2737], Loss: 0.1255\n",
            "Epoch [4/10], Step [2500/2737], Loss: 0.5479\n",
            "Epoch [4/10], Step [2600/2737], Loss: 0.1059\n",
            "Epoch [4/10], Step [2700/2737], Loss: 0.1369\n",
            "Epoch [5/10], Step [100/2737], Loss: 0.2184\n",
            "Epoch [5/10], Step [200/2737], Loss: 0.1348\n",
            "Epoch [5/10], Step [300/2737], Loss: 0.0785\n",
            "Epoch [5/10], Step [400/2737], Loss: 0.2260\n",
            "Epoch [5/10], Step [500/2737], Loss: 0.0381\n",
            "Epoch [5/10], Step [600/2737], Loss: 0.2208\n",
            "Epoch [5/10], Step [700/2737], Loss: 0.1132\n",
            "Epoch [5/10], Step [800/2737], Loss: 0.0276\n",
            "Epoch [5/10], Step [900/2737], Loss: 0.1345\n",
            "Epoch [5/10], Step [1000/2737], Loss: 0.1812\n",
            "Epoch [5/10], Step [1100/2737], Loss: 0.3068\n",
            "Epoch [5/10], Step [1200/2737], Loss: 0.3490\n",
            "Epoch [5/10], Step [1300/2737], Loss: 0.0280\n",
            "Epoch [5/10], Step [1400/2737], Loss: 0.2075\n",
            "Epoch [5/10], Step [1500/2737], Loss: 0.0186\n",
            "Epoch [5/10], Step [1600/2737], Loss: 0.0910\n",
            "Epoch [5/10], Step [1700/2737], Loss: 0.0146\n",
            "Epoch [5/10], Step [1800/2737], Loss: 0.0680\n",
            "Epoch [5/10], Step [1900/2737], Loss: 0.1474\n",
            "Epoch [5/10], Step [2000/2737], Loss: 0.3282\n",
            "Epoch [5/10], Step [2100/2737], Loss: 0.2127\n",
            "Epoch [5/10], Step [2200/2737], Loss: 0.2655\n",
            "Epoch [5/10], Step [2300/2737], Loss: 0.0413\n",
            "Epoch [5/10], Step [2400/2737], Loss: 0.0383\n",
            "Epoch [5/10], Step [2500/2737], Loss: 0.1265\n",
            "Epoch [5/10], Step [2600/2737], Loss: 0.1398\n",
            "Epoch [5/10], Step [2700/2737], Loss: 0.0759\n",
            "Epoch [6/10], Step [100/2737], Loss: 0.0332\n",
            "Epoch [6/10], Step [200/2737], Loss: 0.1040\n",
            "Epoch [6/10], Step [300/2737], Loss: 0.2795\n",
            "Epoch [6/10], Step [400/2737], Loss: 0.3311\n",
            "Epoch [6/10], Step [500/2737], Loss: 0.1619\n",
            "Epoch [6/10], Step [600/2737], Loss: 0.0355\n",
            "Epoch [6/10], Step [700/2737], Loss: 0.1745\n",
            "Epoch [6/10], Step [800/2737], Loss: 0.3519\n",
            "Epoch [6/10], Step [900/2737], Loss: 0.0471\n",
            "Epoch [6/10], Step [1000/2737], Loss: 0.0361\n",
            "Epoch [6/10], Step [1100/2737], Loss: 0.1123\n",
            "Epoch [6/10], Step [1200/2737], Loss: 0.0788\n",
            "Epoch [6/10], Step [1300/2737], Loss: 0.2138\n",
            "Epoch [6/10], Step [1400/2737], Loss: 0.0342\n",
            "Epoch [6/10], Step [1500/2737], Loss: 0.2352\n",
            "Epoch [6/10], Step [1600/2737], Loss: 0.0132\n",
            "Epoch [6/10], Step [1700/2737], Loss: 0.1639\n",
            "Epoch [6/10], Step [1800/2737], Loss: 0.0520\n",
            "Epoch [6/10], Step [1900/2737], Loss: 0.0297\n",
            "Epoch [6/10], Step [2000/2737], Loss: 0.3355\n",
            "Epoch [6/10], Step [2100/2737], Loss: 0.0135\n",
            "Epoch [6/10], Step [2200/2737], Loss: 0.0630\n",
            "Epoch [6/10], Step [2300/2737], Loss: 0.0538\n",
            "Epoch [6/10], Step [2400/2737], Loss: 0.0891\n",
            "Epoch [6/10], Step [2500/2737], Loss: 0.0620\n",
            "Epoch [6/10], Step [2600/2737], Loss: 0.0280\n",
            "Epoch [6/10], Step [2700/2737], Loss: 0.3196\n",
            "Epoch [7/10], Step [100/2737], Loss: 0.0485\n",
            "Epoch [7/10], Step [200/2737], Loss: 0.1001\n",
            "Epoch [7/10], Step [300/2737], Loss: 0.0422\n",
            "Epoch [7/10], Step [400/2737], Loss: 0.2539\n",
            "Epoch [7/10], Step [500/2737], Loss: 0.1125\n",
            "Epoch [7/10], Step [600/2737], Loss: 0.0306\n",
            "Epoch [7/10], Step [700/2737], Loss: 0.0267\n",
            "Epoch [7/10], Step [800/2737], Loss: 0.0180\n",
            "Epoch [7/10], Step [900/2737], Loss: 0.1578\n",
            "Epoch [7/10], Step [1000/2737], Loss: 0.0210\n",
            "Epoch [7/10], Step [1100/2737], Loss: 0.2407\n",
            "Epoch [7/10], Step [1200/2737], Loss: 0.0822\n",
            "Epoch [7/10], Step [1300/2737], Loss: 0.1260\n",
            "Epoch [7/10], Step [1400/2737], Loss: 0.0081\n",
            "Epoch [7/10], Step [1500/2737], Loss: 0.0181\n",
            "Epoch [7/10], Step [1600/2737], Loss: 0.2319\n",
            "Epoch [7/10], Step [1700/2737], Loss: 0.1399\n",
            "Epoch [7/10], Step [1800/2737], Loss: 0.0181\n",
            "Epoch [7/10], Step [1900/2737], Loss: 0.2901\n",
            "Epoch [7/10], Step [2000/2737], Loss: 0.0310\n",
            "Epoch [7/10], Step [2100/2737], Loss: 0.2490\n",
            "Epoch [7/10], Step [2200/2737], Loss: 0.2561\n",
            "Epoch [7/10], Step [2300/2737], Loss: 0.0768\n",
            "Epoch [7/10], Step [2400/2737], Loss: 0.1465\n",
            "Epoch [7/10], Step [2500/2737], Loss: 0.2378\n",
            "Epoch [7/10], Step [2600/2737], Loss: 0.1496\n",
            "Epoch [7/10], Step [2700/2737], Loss: 0.0252\n",
            "Epoch [8/10], Step [100/2737], Loss: 0.0468\n",
            "Epoch [8/10], Step [200/2737], Loss: 0.1061\n",
            "Epoch [8/10], Step [300/2737], Loss: 0.0279\n",
            "Epoch [8/10], Step [400/2737], Loss: 0.0211\n",
            "Epoch [8/10], Step [500/2737], Loss: 0.1304\n",
            "Epoch [8/10], Step [600/2737], Loss: 0.1349\n",
            "Epoch [8/10], Step [700/2737], Loss: 0.1468\n",
            "Epoch [8/10], Step [800/2737], Loss: 0.0590\n",
            "Epoch [8/10], Step [900/2737], Loss: 0.2893\n",
            "Epoch [8/10], Step [1000/2737], Loss: 0.1117\n",
            "Epoch [8/10], Step [1100/2737], Loss: 0.0120\n",
            "Epoch [8/10], Step [1200/2737], Loss: 0.1596\n",
            "Epoch [8/10], Step [1300/2737], Loss: 0.1181\n",
            "Epoch [8/10], Step [1400/2737], Loss: 0.0347\n",
            "Epoch [8/10], Step [1500/2737], Loss: 0.1256\n",
            "Epoch [8/10], Step [1600/2737], Loss: 0.0686\n",
            "Epoch [8/10], Step [1700/2737], Loss: 0.1251\n",
            "Epoch [8/10], Step [1800/2737], Loss: 0.0653\n",
            "Epoch [8/10], Step [1900/2737], Loss: 0.0331\n",
            "Epoch [8/10], Step [2000/2737], Loss: 0.0161\n",
            "Epoch [8/10], Step [2100/2737], Loss: 0.0474\n",
            "Epoch [8/10], Step [2200/2737], Loss: 0.0987\n",
            "Epoch [8/10], Step [2300/2737], Loss: 0.0179\n",
            "Epoch [8/10], Step [2400/2737], Loss: 0.0117\n",
            "Epoch [8/10], Step [2500/2737], Loss: 0.2577\n",
            "Epoch [8/10], Step [2600/2737], Loss: 0.0960\n",
            "Epoch [8/10], Step [2700/2737], Loss: 0.0264\n",
            "Epoch [9/10], Step [100/2737], Loss: 0.1053\n",
            "Epoch [9/10], Step [200/2737], Loss: 0.1408\n",
            "Epoch [9/10], Step [300/2737], Loss: 0.1427\n",
            "Epoch [9/10], Step [400/2737], Loss: 0.0464\n",
            "Epoch [9/10], Step [500/2737], Loss: 0.0291\n",
            "Epoch [9/10], Step [600/2737], Loss: 0.0150\n",
            "Epoch [9/10], Step [700/2737], Loss: 0.0325\n",
            "Epoch [9/10], Step [800/2737], Loss: 0.0146\n",
            "Epoch [9/10], Step [900/2737], Loss: 0.0237\n",
            "Epoch [9/10], Step [1000/2737], Loss: 0.0513\n",
            "Epoch [9/10], Step [1100/2737], Loss: 0.0119\n",
            "Epoch [9/10], Step [1200/2737], Loss: 0.0076\n",
            "Epoch [9/10], Step [1300/2737], Loss: 0.1949\n",
            "Epoch [9/10], Step [1400/2737], Loss: 0.0412\n",
            "Epoch [9/10], Step [1500/2737], Loss: 0.1474\n",
            "Epoch [9/10], Step [1600/2737], Loss: 0.0205\n",
            "Epoch [9/10], Step [1700/2737], Loss: 0.2728\n",
            "Epoch [9/10], Step [1800/2737], Loss: 0.0467\n",
            "Epoch [9/10], Step [1900/2737], Loss: 0.1031\n",
            "Epoch [9/10], Step [2000/2737], Loss: 0.1404\n",
            "Epoch [9/10], Step [2100/2737], Loss: 0.2417\n",
            "Epoch [9/10], Step [2200/2737], Loss: 0.0393\n",
            "Epoch [9/10], Step [2300/2737], Loss: 0.0390\n",
            "Epoch [9/10], Step [2400/2737], Loss: 0.1333\n",
            "Epoch [9/10], Step [2500/2737], Loss: 0.1680\n",
            "Epoch [9/10], Step [2600/2737], Loss: 0.2503\n",
            "Epoch [9/10], Step [2700/2737], Loss: 0.0302\n",
            "Epoch [10/10], Step [100/2737], Loss: 0.0603\n",
            "Epoch [10/10], Step [200/2737], Loss: 0.1668\n",
            "Epoch [10/10], Step [300/2737], Loss: 0.0145\n",
            "Epoch [10/10], Step [400/2737], Loss: 0.0098\n",
            "Epoch [10/10], Step [500/2737], Loss: 0.0340\n",
            "Epoch [10/10], Step [600/2737], Loss: 0.0776\n",
            "Epoch [10/10], Step [700/2737], Loss: 0.0659\n",
            "Epoch [10/10], Step [800/2737], Loss: 0.0068\n",
            "Epoch [10/10], Step [900/2737], Loss: 0.0361\n",
            "Epoch [10/10], Step [1000/2737], Loss: 0.0190\n",
            "Epoch [10/10], Step [1100/2737], Loss: 0.0465\n",
            "Epoch [10/10], Step [1200/2737], Loss: 0.0409\n",
            "Epoch [10/10], Step [1300/2737], Loss: 0.3489\n",
            "Epoch [10/10], Step [1400/2737], Loss: 0.0451\n",
            "Epoch [10/10], Step [1500/2737], Loss: 0.1803\n",
            "Epoch [10/10], Step [1600/2737], Loss: 0.0300\n",
            "Epoch [10/10], Step [1700/2737], Loss: 0.0114\n",
            "Epoch [10/10], Step [1800/2737], Loss: 0.0127\n",
            "Epoch [10/10], Step [1900/2737], Loss: 0.0730\n",
            "Epoch [10/10], Step [2000/2737], Loss: 0.0177\n",
            "Epoch [10/10], Step [2100/2737], Loss: 0.1829\n",
            "Epoch [10/10], Step [2200/2737], Loss: 0.2681\n",
            "Epoch [10/10], Step [2300/2737], Loss: 0.3337\n",
            "Epoch [10/10], Step [2400/2737], Loss: 0.0104\n",
            "Epoch [10/10], Step [2500/2737], Loss: 0.1679\n",
            "Epoch [10/10], Step [2600/2737], Loss: 0.2059\n",
            "Epoch [10/10], Step [2700/2737], Loss: 0.0341\n",
            "Accuracy: 97.49%\n",
            "Precision: 0.9741\n",
            "Recall: 0.9749\n",
            "F1 Score: 0.9724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM with Attention Experiment\n",
        "### Description\n",
        "Use the ECG dataset to train the LSTM+Attention model and evaluate its performance.\n",
        "\n",
        "### Steps\n",
        "1. set up the LSTM+Attention model\n",
        "1. train the LSTM+Attention model\n",
        "1. evaluate performance with accuracy, F1 score, precision, recall"
      ],
      "metadata": {
        "id": "8H-W0Bxa4MQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd tsc_attention\n",
        "\n",
        "# set parameters\n",
        "input_dim = 1\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = 5\n",
        "\n",
        "attention_model = AttentionLSTM(input_dim, hidden_dim, num_layers, num_classes)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(attention_model.parameters(), lr=0.001)\n",
        "\n",
        "print(attention_model)"
      ],
      "metadata": {
        "id": "v7i9kqLcyKK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6e29bc-0270-44e1-dc3d-2b4aa6914b25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttentionLSTM(\n",
            "  (lstm): LSTM(1, 128, num_layers=2, batch_first=True)\n",
            "  (attention): SelfAttention(\n",
            "    (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    attention_model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = attention_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# evaluate\n",
        "attention_model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = attention_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        true_labels.append(labels)\n",
        "        predicted_labels.append(predicted)\n",
        "\n",
        "# combine lists of results\n",
        "true_labels = torch.cat(true_labels).cpu().numpy()\n",
        "predicted_labels = torch.cat(predicted_labels).cpu().numpy()\n",
        "\n",
        "# metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "recall = recall_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "r7Ns3KEP4ZbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff72b6f-d9df-42a6-8e39-032646f23bcb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/2737], Loss: 0.9986\n",
            "Epoch [1/10], Step [200/2737], Loss: 1.1548\n",
            "Epoch [1/10], Step [300/2737], Loss: 1.0923\n",
            "Epoch [1/10], Step [400/2737], Loss: 1.1236\n",
            "Epoch [1/10], Step [500/2737], Loss: 1.1548\n",
            "Epoch [1/10], Step [600/2737], Loss: 1.0923\n",
            "Epoch [1/10], Step [700/2737], Loss: 1.1236\n",
            "Epoch [1/10], Step [800/2737], Loss: 1.0298\n",
            "Epoch [1/10], Step [900/2737], Loss: 1.1861\n",
            "Epoch [1/10], Step [1000/2737], Loss: 1.0611\n",
            "Epoch [1/10], Step [1100/2737], Loss: 0.9361\n",
            "Epoch [1/10], Step [1200/2737], Loss: 1.0611\n",
            "Epoch [1/10], Step [1300/2737], Loss: 1.0923\n",
            "Epoch [1/10], Step [1400/2737], Loss: 0.9361\n",
            "Epoch [1/10], Step [1500/2737], Loss: 1.0298\n",
            "Epoch [1/10], Step [1600/2737], Loss: 1.2173\n",
            "Epoch [1/10], Step [1700/2737], Loss: 1.0923\n",
            "Epoch [1/10], Step [1800/2737], Loss: 1.0298\n",
            "Epoch [1/10], Step [1900/2737], Loss: 0.9986\n",
            "Epoch [1/10], Step [2000/2737], Loss: 0.9986\n",
            "Epoch [1/10], Step [2100/2737], Loss: 1.1861\n",
            "Epoch [1/10], Step [2200/2737], Loss: 1.0298\n",
            "Epoch [1/10], Step [2300/2737], Loss: 1.0611\n",
            "Epoch [1/10], Step [2400/2737], Loss: 1.1236\n",
            "Epoch [1/10], Step [2500/2737], Loss: 1.0923\n",
            "Epoch [1/10], Step [2600/2737], Loss: 1.0611\n",
            "Epoch [1/10], Step [2700/2737], Loss: 1.1861\n",
            "Epoch [2/10], Step [100/2737], Loss: 1.1236\n",
            "Epoch [2/10], Step [200/2737], Loss: 0.9986\n",
            "Epoch [2/10], Step [300/2737], Loss: 1.0923\n",
            "Epoch [2/10], Step [400/2737], Loss: 1.1861\n",
            "Epoch [2/10], Step [500/2737], Loss: 1.0611\n",
            "Epoch [2/10], Step [600/2737], Loss: 1.2486\n",
            "Epoch [2/10], Step [700/2737], Loss: 1.0298\n",
            "Epoch [2/10], Step [800/2737], Loss: 1.0611\n",
            "Epoch [2/10], Step [900/2737], Loss: 1.1236\n",
            "Epoch [2/10], Step [1000/2737], Loss: 1.1236\n",
            "Epoch [2/10], Step [1100/2737], Loss: 0.9986\n",
            "Epoch [2/10], Step [1200/2737], Loss: 1.0923\n",
            "Epoch [2/10], Step [1300/2737], Loss: 1.0611\n",
            "Epoch [2/10], Step [1400/2737], Loss: 0.9986\n",
            "Epoch [2/10], Step [1500/2737], Loss: 1.1548\n",
            "Epoch [2/10], Step [1600/2737], Loss: 1.1236\n",
            "Epoch [2/10], Step [1700/2737], Loss: 1.0923\n",
            "Epoch [2/10], Step [1800/2737], Loss: 1.0923\n",
            "Epoch [2/10], Step [1900/2737], Loss: 1.1236\n",
            "Epoch [2/10], Step [2000/2737], Loss: 1.0611\n",
            "Epoch [2/10], Step [2100/2737], Loss: 1.0923\n",
            "Epoch [2/10], Step [2200/2737], Loss: 1.0611\n",
            "Epoch [2/10], Step [2300/2737], Loss: 1.0923\n",
            "Epoch [2/10], Step [2400/2737], Loss: 1.0611\n",
            "Epoch [2/10], Step [2500/2737], Loss: 1.0923\n",
            "Epoch [2/10], Step [2600/2737], Loss: 1.0298\n",
            "Epoch [2/10], Step [2700/2737], Loss: 1.0923\n",
            "Epoch [3/10], Step [100/2737], Loss: 1.0611\n",
            "Epoch [3/10], Step [200/2737], Loss: 0.9986\n",
            "Epoch [3/10], Step [300/2737], Loss: 1.0298\n",
            "Epoch [3/10], Step [400/2737], Loss: 1.0923\n",
            "Epoch [3/10], Step [500/2737], Loss: 1.0611\n",
            "Epoch [3/10], Step [600/2737], Loss: 1.1236\n",
            "Epoch [3/10], Step [700/2737], Loss: 1.0923\n",
            "Epoch [3/10], Step [800/2737], Loss: 1.1236\n",
            "Epoch [3/10], Step [900/2737], Loss: 1.0611\n",
            "Epoch [3/10], Step [1000/2737], Loss: 1.1236\n",
            "Epoch [3/10], Step [1100/2737], Loss: 1.0923\n",
            "Epoch [3/10], Step [1200/2737], Loss: 1.0611\n",
            "Epoch [3/10], Step [1300/2737], Loss: 0.9986\n",
            "Epoch [3/10], Step [1400/2737], Loss: 1.0611\n",
            "Epoch [3/10], Step [1500/2737], Loss: 1.1236\n",
            "Epoch [3/10], Step [1600/2737], Loss: 1.1548\n",
            "Epoch [3/10], Step [1700/2737], Loss: 1.0611\n",
            "Epoch [3/10], Step [1800/2737], Loss: 0.9986\n",
            "Epoch [3/10], Step [1900/2737], Loss: 1.0923\n",
            "Epoch [3/10], Step [2000/2737], Loss: 1.1236\n",
            "Epoch [3/10], Step [2100/2737], Loss: 1.0923\n",
            "Epoch [3/10], Step [2200/2737], Loss: 1.0298\n",
            "Epoch [3/10], Step [2300/2737], Loss: 1.1548\n",
            "Epoch [3/10], Step [2400/2737], Loss: 1.1236\n",
            "Epoch [3/10], Step [2500/2737], Loss: 1.1861\n",
            "Epoch [3/10], Step [2600/2737], Loss: 1.1548\n",
            "Epoch [3/10], Step [2700/2737], Loss: 1.0923\n",
            "Epoch [4/10], Step [100/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [200/2737], Loss: 1.1236\n",
            "Epoch [4/10], Step [300/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [400/2737], Loss: 1.1548\n",
            "Epoch [4/10], Step [500/2737], Loss: 1.1236\n",
            "Epoch [4/10], Step [600/2737], Loss: 0.9673\n",
            "Epoch [4/10], Step [700/2737], Loss: 1.0298\n",
            "Epoch [4/10], Step [800/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [900/2737], Loss: 1.0923\n",
            "Epoch [4/10], Step [1000/2737], Loss: 0.9673\n",
            "Epoch [4/10], Step [1100/2737], Loss: 1.0923\n",
            "Epoch [4/10], Step [1200/2737], Loss: 1.2173\n",
            "Epoch [4/10], Step [1300/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [1400/2737], Loss: 1.1236\n",
            "Epoch [4/10], Step [1500/2737], Loss: 0.9986\n",
            "Epoch [4/10], Step [1600/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [1700/2737], Loss: 1.1236\n",
            "Epoch [4/10], Step [1800/2737], Loss: 1.1236\n",
            "Epoch [4/10], Step [1900/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [2000/2737], Loss: 1.2173\n",
            "Epoch [4/10], Step [2100/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [2200/2737], Loss: 1.0923\n",
            "Epoch [4/10], Step [2300/2737], Loss: 1.1236\n",
            "Epoch [4/10], Step [2400/2737], Loss: 1.1236\n",
            "Epoch [4/10], Step [2500/2737], Loss: 0.9048\n",
            "Epoch [4/10], Step [2600/2737], Loss: 1.0611\n",
            "Epoch [4/10], Step [2700/2737], Loss: 0.9361\n",
            "Epoch [5/10], Step [100/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [200/2737], Loss: 0.9673\n",
            "Epoch [5/10], Step [300/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [400/2737], Loss: 0.9673\n",
            "Epoch [5/10], Step [500/2737], Loss: 1.1861\n",
            "Epoch [5/10], Step [600/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [700/2737], Loss: 1.1236\n",
            "Epoch [5/10], Step [800/2737], Loss: 1.0298\n",
            "Epoch [5/10], Step [900/2737], Loss: 0.9986\n",
            "Epoch [5/10], Step [1000/2737], Loss: 1.1236\n",
            "Epoch [5/10], Step [1100/2737], Loss: 1.0298\n",
            "Epoch [5/10], Step [1200/2737], Loss: 1.1236\n",
            "Epoch [5/10], Step [1300/2737], Loss: 0.9986\n",
            "Epoch [5/10], Step [1400/2737], Loss: 1.1548\n",
            "Epoch [5/10], Step [1500/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [1600/2737], Loss: 1.1548\n",
            "Epoch [5/10], Step [1700/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [1800/2737], Loss: 1.0923\n",
            "Epoch [5/10], Step [1900/2737], Loss: 1.1236\n",
            "Epoch [5/10], Step [2000/2737], Loss: 0.9673\n",
            "Epoch [5/10], Step [2100/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [2200/2737], Loss: 0.9986\n",
            "Epoch [5/10], Step [2300/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [2400/2737], Loss: 1.1236\n",
            "Epoch [5/10], Step [2500/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [2600/2737], Loss: 1.0611\n",
            "Epoch [5/10], Step [2700/2737], Loss: 0.9986\n",
            "Epoch [6/10], Step [100/2737], Loss: 1.0298\n",
            "Epoch [6/10], Step [200/2737], Loss: 1.0923\n",
            "Epoch [6/10], Step [300/2737], Loss: 1.0298\n",
            "Epoch [6/10], Step [400/2737], Loss: 1.0611\n",
            "Epoch [6/10], Step [500/2737], Loss: 0.9986\n",
            "Epoch [6/10], Step [600/2737], Loss: 1.1236\n",
            "Epoch [6/10], Step [700/2737], Loss: 1.3111\n",
            "Epoch [6/10], Step [800/2737], Loss: 1.1236\n",
            "Epoch [6/10], Step [900/2737], Loss: 1.0923\n",
            "Epoch [6/10], Step [1000/2737], Loss: 1.0611\n",
            "Epoch [6/10], Step [1100/2737], Loss: 1.1236\n",
            "Epoch [6/10], Step [1200/2737], Loss: 0.9673\n",
            "Epoch [6/10], Step [1300/2737], Loss: 1.1236\n",
            "Epoch [6/10], Step [1400/2737], Loss: 1.1236\n",
            "Epoch [6/10], Step [1500/2737], Loss: 1.0923\n",
            "Epoch [6/10], Step [1600/2737], Loss: 1.1236\n",
            "Epoch [6/10], Step [1700/2737], Loss: 0.9361\n",
            "Epoch [6/10], Step [1800/2737], Loss: 1.0923\n",
            "Epoch [6/10], Step [1900/2737], Loss: 0.9673\n",
            "Epoch [6/10], Step [2000/2737], Loss: 1.0611\n",
            "Epoch [6/10], Step [2100/2737], Loss: 1.1236\n",
            "Epoch [6/10], Step [2200/2737], Loss: 1.2173\n",
            "Epoch [6/10], Step [2300/2737], Loss: 0.9673\n",
            "Epoch [6/10], Step [2400/2737], Loss: 1.0298\n",
            "Epoch [6/10], Step [2500/2737], Loss: 0.9673\n",
            "Epoch [6/10], Step [2600/2737], Loss: 1.0298\n",
            "Epoch [6/10], Step [2700/2737], Loss: 1.0923\n",
            "Epoch [7/10], Step [100/2737], Loss: 1.0298\n",
            "Epoch [7/10], Step [200/2737], Loss: 1.0611\n",
            "Epoch [7/10], Step [300/2737], Loss: 1.1236\n",
            "Epoch [7/10], Step [400/2737], Loss: 1.0298\n",
            "Epoch [7/10], Step [500/2737], Loss: 1.0298\n",
            "Epoch [7/10], Step [600/2737], Loss: 1.0611\n",
            "Epoch [7/10], Step [700/2737], Loss: 1.0298\n",
            "Epoch [7/10], Step [800/2737], Loss: 1.1861\n",
            "Epoch [7/10], Step [900/2737], Loss: 1.1548\n",
            "Epoch [7/10], Step [1000/2737], Loss: 1.1861\n",
            "Epoch [7/10], Step [1100/2737], Loss: 1.0611\n",
            "Epoch [7/10], Step [1200/2737], Loss: 0.9673\n",
            "Epoch [7/10], Step [1300/2737], Loss: 0.9673\n",
            "Epoch [7/10], Step [1400/2737], Loss: 1.0611\n",
            "Epoch [7/10], Step [1500/2737], Loss: 1.1861\n",
            "Epoch [7/10], Step [1600/2737], Loss: 1.1548\n",
            "Epoch [7/10], Step [1700/2737], Loss: 1.0611\n",
            "Epoch [7/10], Step [1800/2737], Loss: 1.0298\n",
            "Epoch [7/10], Step [1900/2737], Loss: 1.1236\n",
            "Epoch [7/10], Step [2000/2737], Loss: 0.9986\n",
            "Epoch [7/10], Step [2100/2737], Loss: 1.1236\n",
            "Epoch [7/10], Step [2200/2737], Loss: 1.0298\n",
            "Epoch [7/10], Step [2300/2737], Loss: 1.0298\n",
            "Epoch [7/10], Step [2400/2737], Loss: 1.3111\n",
            "Epoch [7/10], Step [2500/2737], Loss: 1.0923\n",
            "Epoch [7/10], Step [2600/2737], Loss: 1.1236\n",
            "Epoch [7/10], Step [2700/2737], Loss: 0.9361\n",
            "Epoch [8/10], Step [100/2737], Loss: 1.0923\n",
            "Epoch [8/10], Step [200/2737], Loss: 0.9361\n",
            "Epoch [8/10], Step [300/2737], Loss: 1.2486\n",
            "Epoch [8/10], Step [400/2737], Loss: 1.0611\n",
            "Epoch [8/10], Step [500/2737], Loss: 1.1236\n",
            "Epoch [8/10], Step [600/2737], Loss: 1.1236\n",
            "Epoch [8/10], Step [700/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [800/2737], Loss: 1.0923\n",
            "Epoch [8/10], Step [900/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [1000/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [1100/2737], Loss: 1.1861\n",
            "Epoch [8/10], Step [1200/2737], Loss: 0.9361\n",
            "Epoch [8/10], Step [1300/2737], Loss: 1.0923\n",
            "Epoch [8/10], Step [1400/2737], Loss: 1.1236\n",
            "Epoch [8/10], Step [1500/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [1600/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [1700/2737], Loss: 1.0611\n",
            "Epoch [8/10], Step [1800/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [1900/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [2000/2737], Loss: 1.0923\n",
            "Epoch [8/10], Step [2100/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [2200/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [2300/2737], Loss: 1.0923\n",
            "Epoch [8/10], Step [2400/2737], Loss: 1.0298\n",
            "Epoch [8/10], Step [2500/2737], Loss: 1.0923\n",
            "Epoch [8/10], Step [2600/2737], Loss: 0.9986\n",
            "Epoch [8/10], Step [2700/2737], Loss: 1.0611\n",
            "Epoch [9/10], Step [100/2737], Loss: 1.0298\n",
            "Epoch [9/10], Step [200/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [300/2737], Loss: 1.1548\n",
            "Epoch [9/10], Step [400/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [500/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [600/2737], Loss: 1.0611\n",
            "Epoch [9/10], Step [700/2737], Loss: 1.1236\n",
            "Epoch [9/10], Step [800/2737], Loss: 0.9361\n",
            "Epoch [9/10], Step [900/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [1000/2737], Loss: 1.1236\n",
            "Epoch [9/10], Step [1100/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [1200/2737], Loss: 1.1548\n",
            "Epoch [9/10], Step [1300/2737], Loss: 0.9673\n",
            "Epoch [9/10], Step [1400/2737], Loss: 1.2173\n",
            "Epoch [9/10], Step [1500/2737], Loss: 1.0298\n",
            "Epoch [9/10], Step [1600/2737], Loss: 1.1548\n",
            "Epoch [9/10], Step [1700/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [1800/2737], Loss: 1.1861\n",
            "Epoch [9/10], Step [1900/2737], Loss: 1.1548\n",
            "Epoch [9/10], Step [2000/2737], Loss: 1.0298\n",
            "Epoch [9/10], Step [2100/2737], Loss: 1.1236\n",
            "Epoch [9/10], Step [2200/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [2300/2737], Loss: 1.0923\n",
            "Epoch [9/10], Step [2400/2737], Loss: 1.2173\n",
            "Epoch [9/10], Step [2500/2737], Loss: 1.0611\n",
            "Epoch [9/10], Step [2600/2737], Loss: 1.0298\n",
            "Epoch [9/10], Step [2700/2737], Loss: 1.0611\n",
            "Epoch [10/10], Step [100/2737], Loss: 1.1548\n",
            "Epoch [10/10], Step [200/2737], Loss: 0.9986\n",
            "Epoch [10/10], Step [300/2737], Loss: 0.9673\n",
            "Epoch [10/10], Step [400/2737], Loss: 1.0298\n",
            "Epoch [10/10], Step [500/2737], Loss: 1.0923\n",
            "Epoch [10/10], Step [600/2737], Loss: 1.0923\n",
            "Epoch [10/10], Step [700/2737], Loss: 1.0298\n",
            "Epoch [10/10], Step [800/2737], Loss: 1.1236\n",
            "Epoch [10/10], Step [900/2737], Loss: 1.0923\n",
            "Epoch [10/10], Step [1000/2737], Loss: 0.9986\n",
            "Epoch [10/10], Step [1100/2737], Loss: 1.0611\n",
            "Epoch [10/10], Step [1200/2737], Loss: 1.1236\n",
            "Epoch [10/10], Step [1300/2737], Loss: 0.9673\n",
            "Epoch [10/10], Step [1400/2737], Loss: 0.9986\n",
            "Epoch [10/10], Step [1500/2737], Loss: 0.9986\n",
            "Epoch [10/10], Step [1600/2737], Loss: 1.0923\n",
            "Epoch [10/10], Step [1700/2737], Loss: 0.9986\n",
            "Epoch [10/10], Step [1800/2737], Loss: 1.1548\n",
            "Epoch [10/10], Step [1900/2737], Loss: 1.0298\n",
            "Epoch [10/10], Step [2000/2737], Loss: 1.0611\n",
            "Epoch [10/10], Step [2100/2737], Loss: 0.9986\n",
            "Epoch [10/10], Step [2200/2737], Loss: 0.9673\n",
            "Epoch [10/10], Step [2300/2737], Loss: 1.0611\n",
            "Epoch [10/10], Step [2400/2737], Loss: 1.0611\n",
            "Epoch [10/10], Step [2500/2737], Loss: 1.1861\n",
            "Epoch [10/10], Step [2600/2737], Loss: 0.9673\n",
            "Epoch [10/10], Step [2700/2737], Loss: 1.0298\n",
            "Accuracy: 82.76%\n",
            "Precision: 0.6849\n",
            "Recall: 0.8276\n",
            "F1 Score: 0.7495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}
